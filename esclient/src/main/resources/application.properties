spring.data.elasticsearch.cluster-nodes=127.0.0.1:9200
# 多个节点用逗号分隔

#spring.data.elasticsearch.cluster-name=JGMa-ESAPP
spring.data.elasticsearch.cluster-name=elasticsearch
spring.profiles.active=dev

#系统参数
app.transport_es_nodes=127.0.0.1:9300
app.is_SSL_es=false


#============== kafka手动配置信息 ===================
# 指定kafka 代理地址，可以多个
kafka.bootstrap-servers=127.0.0.1:9092

#=============== provider  =======================

#发送失败后的重复发送次数
kafka.producer.retries=0
# 每次批量发送消息的数量
kafka.producer.batch-size=16384
#32M批处理缓冲区
kafka.producer.buffer-memory=33554432

# 指定消息key和消息体的编解码方式
kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer

#=============== consumer  =======================
# 指定默认消费者group id
kafk.aconsumer.group-id=test-consumer-group
#kafk.aconsumer.group-id=elasticLocalTest

#earliest
#当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
#latest
#当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
#none
#topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
kafka.consumer.auto-offset-reset=earliest

#设置手动提交，但是不配置下面这个，spring会帮助自动提交
#factory.getContainerProperties().setAckMode(AbstractMessageListenerContainer.AckMode.MANUAL_IMMEDIATE);
kafka.consumer.enable-auto-commit=false

#自动提交的间隔时间,
kafka.consumer.auto-commit-interval=100

# 指定消息key和消息体的编解码方式
kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer

# 设置启用批量消费
kafka.consumer.batch.listener=true

#设置批量消费参数
#如果消息队列中没有消息，等待timeout毫秒后，调用poll()方法。
#如果队列中有消息，立即消费消息，处理这些消息所用时间不能超过max.pool.interval.ms值。即：两次poll的时间间隔最大时间，否则抛CommitFailedException
#每次消费的消息的最大数可以通过max.poll.records配置

#批量消费一次最大拉取的数据量
kafka.consumer.batch.max.pool.records=100

#手动提交设置与poll的心跳数,不能设置过高重复消费
kafka.consumer.batch.max.pool.interval.ms=3000
kafka.consumer.batch.max.pool.timeout.ms=1500

#连接超时时间
kafka.consumer.session.timeout=6000

#自动提交的间隔时间
#kafka.consumer.auto.commit.interval=100
